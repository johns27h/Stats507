{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deca5039",
   "metadata": {},
   "source": [
    "## Topics in Pandas\n",
    "**Stats 507, Fall 2021** \n",
    "\n",
    "**Group 0**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d54bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9cd2a9",
   "metadata": {},
   "source": [
    "## Contents\n",
    "Add a bullet for each topic and link to the level 2 title header using \n",
    "the exact title with spaces replaced by a dash. \n",
    "\n",
    "+ [Pivot tables](#Pivot-tables)\n",
    "+ [One row to many](#One-row-to-many)\n",
    "+ [DataFrame.pct_change()](#DataFrame.pct_change()) \n",
    "+ [Working with missing data](#Working-with-missing-data)\n",
    "+ [Cumulative sums](#Title:-pandas.DataFrame.cumsum)\n",
    "+ [Stack and unstack](#Stack-and-unstack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc58b7e",
   "metadata": {},
   "source": [
    "## Pivot tables\n",
    "Zeyuan Li\n",
    "zeyuanli@umich.edu\n",
    "10/19/2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2db066",
   "metadata": {},
   "source": [
    "## Pivot tables in pandas\n",
    "\n",
    "The pivot tables in Excel is very powerful and convienent in handling with numeric data. Pandas also provides ```pivot_table()``` for pivoting with aggregation of numeric data. There are 5 main arguments of ```pivot_table()```:\n",
    "* ***data***: a DataFrame object\n",
    "* ***values***: a column or a list of columns to aggregate.\n",
    "* ***index***: Keys to group by on the pivot table index. \n",
    "* ***columns***:  Keys to group by on the pivot table column. \n",
    "* ***aggfunc***: function to use for aggregation, defaulting to ```numpy.mean```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734a81e",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5b17b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"one\", \"one\", \"two\", \"three\"] * 6,\n",
    "        \"B\": [\"A\", \"B\", \"C\"] * 8,\n",
    "        \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 4,\n",
    "        \"D\": np.random.randn(24),\n",
    "        \"E\": np.random.randn(24),\n",
    "        \"F\": [datetime.datetime(2013, i, 1) for i in range(1, 13)]\n",
    "        + [datetime.datetime(2013, i, 15) for i in range(1, 13)],\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d825b3e",
   "metadata": {},
   "source": [
    "### Do aggregation\n",
    "\n",
    "* Get the pivot table easily. \n",
    "* Produce the table as the same result of doing ```groupby(['A','B','C'])``` and compute the ```mean``` of D, with different values of D shown in seperate columns.\n",
    "* Change to another ***aggfunc*** to finish the aggregation as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5abd77",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=[\"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98196bf6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(df, values=\"D\", index=[\"B\"], columns=[\"A\", \"C\"], aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb618654",
   "metadata": {},
   "source": [
    "### Display all aggregation values\n",
    "\n",
    "* If the ***values*** column name is not given, the pivot table will include all of the data that can be aggregated in an additional level of hierarchy in the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760e4b5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(df, index=[\"A\", \"B\"], columns=[\"C\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93365dd9",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "* You can render a nice output of the table omitting the missing values by calling ```to_string```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(df, index=[\"A\", \"B\"], columns=[\"C\"])\n",
    "print(table.to_string(na_rep=\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a3565",
   "metadata": {},
   "source": [
    "## One row to many\n",
    "\n",
    "*Kunheng Li(kunhengl@umich.edu)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d652d5e0",
   "metadata": {},
   "source": [
    "The reason I choose this function is because last homework. Before the hint from teachers, I found some ways to transfrom one row to many rows. Therefore, I will introduce a function to deal with this type of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399cd30",
   "metadata": {},
   "source": [
    "First, let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ae5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"first name\":[\"kevin\",\"betty\",\"tony\"],\n",
    "    \"last name\":[\"li\",\"jin\",\"zhang\"],\n",
    "    \"courses\":[\"EECS484, STATS507\",\"STATS507, STATS500\",\"EECS402,EECS482,EECS491\"]   \n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index([\"first name\", \"last name\"])[\"courses\"].str.split(\",\", expand=True)\\\n",
    "    .stack().reset_index(drop=True, level=-1).reset_index().rename(columns={0: \"courses\"})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa423ca",
   "metadata": {},
   "source": [
    "This is the first method I want to introduce, stack() or unstack(), both are similar. \n",
    "Unstack() and stack() in DataFrame are to make itself to a Series which has secondary index.\n",
    "Unstack() is to transform its index to secondary index and its column to primary index, however, \n",
    "stack() is to transform its index to primary index and its column to secondary index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe48bf",
   "metadata": {},
   "source": [
    "However, in Pandas 0.25 version, there is a new method in DataFrame called explode(). They have the result, let's see the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc9335d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df[\"courses\"] = df[\"courses\"].str.split(\",\")\n",
    "df = df.explode(\"courses\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9aaf9",
   "metadata": {},
   "source": [
    "## DataFrame.pct_change()\n",
    "*Dongming Yang*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function always be used to calculate the percentage change between the current and a prior element, and always be used to a time series     \n",
    "# The axis could choose the percentage change from row or columns\n",
    "# Creating the time-series index \n",
    "ind = pd.date_range('01/01/2000', periods = 6, freq ='W') \n",
    "  \n",
    "# Creating the dataframe  \n",
    "df = pd.DataFrame({\"A\":[14, 4, 5, 4, 1, 55], \n",
    "                   \"B\":[5, 2, 54, 3, 2, 32],  \n",
    "                   \"C\":[20, 20, 7, 21, 8, 5], \n",
    "                   \"D\":[14, 3, 6, 2, 6, 4]}, index = ind) \n",
    "  \n",
    "# find the percentage change with the previous row \n",
    "df.pct_change()\n",
    "\n",
    "# find the percentage change with precvious columns \n",
    "df.pct_change(axis=1)m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec587bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# periods means start to calculate the percentage change between the periods column or row and the beginning\n",
    "\n",
    "# find the specific percentage change with first row\n",
    "df.pct_change(periods=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_method means the way to handle NAs before computing percentage change by assigning a value to that NAs\n",
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "  \n",
    "# Creating the time-series index \n",
    "ind = pd.date_range('01/01/2000', periods = 6, freq ='W') \n",
    "  \n",
    "# Creating the dataframe  \n",
    "df = pd.DataFrame({\"A\":[14, 4, 5, 4, 1, 55], \n",
    "                   \"B\":[5, 2, None, 3, 2, 32],  \n",
    "                   \"C\":[20, 20, 7, 21, 8, None], \n",
    "                   \"D\":[14, None, 6, 2, 6, 4]}, index = ind) \n",
    "  \n",
    "# apply the pct_change() method \n",
    "# we use the forward fill method to \n",
    "# fill the missing values in the dataframe \n",
    "df.pct_change(fill_method ='ffill')\n",
    "\n",
    "# ## Contents\n",
    "# Add a bullet for each topic and link to the level 2 title header using \n",
    "# the exact title with spaces replaced by a dash. \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163becbf",
   "metadata": {},
   "source": [
    "## Working with missing data\n",
    "*Kailan Xu*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ddc2e",
   "metadata": {},
   "source": [
    "- Detecting missing data\n",
    "- Inserting missing data\n",
    "- Calculations with missing data\n",
    "- Cleaning / filling missing data\n",
    "- Dropping axis labels with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b5d16",
   "metadata": {},
   "source": [
    "### 1. Detecting missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7aa23",
   "metadata": {},
   "source": [
    "As data comes in many shapes and forms, pandas aims to be flexible with regard to handling missing data. While NaN is the default missing value marker for reasons of computational speed and convenience, we need to be able to easily detect this value with data of different types: floating point, integer, boolean, and general object. In many cases, however, the Python None will arise and we wish to also consider that “missing” or “not available” or “NA”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.random.randn(5, 3),\n",
    "    index=[\"a\", \"c\", \"e\", \"f\", \"h\"],\n",
    "    columns=[\"one\", \"two\", \"three\"],\n",
    ")\n",
    "df2 = df.reindex([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3489b",
   "metadata": {},
   "source": [
    "To make detecting missing values easier (and across different array dtypes), pandas provides the `isna()` and `notna()` functions, which are also methods on Series and DataFrame objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11826d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33274689",
   "metadata": {},
   "source": [
    "###  2. Inserting missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7de51",
   "metadata": {},
   "source": [
    "You can insert missing values by simply assigning to containers. The actual missing value used will be chosen based on the dtype.\n",
    "For example, numeric containers will always use NaN regardless of the missing value type chosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1, 2, 3])\n",
    "s.loc[0] = None\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe8c2e",
   "metadata": {},
   "source": [
    "Likewise, datetime containers will always use NaT.\n",
    "For object containers, pandas will use the value given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72204bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([\"a\", \"b\", \"c\"])\n",
    "s.loc[0] = None\n",
    "s.loc[1] = np.nan\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96fdc41",
   "metadata": {},
   "source": [
    "### 3. Calculations with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1beabeb",
   "metadata": {},
   "source": [
    "- When summing data, NA (missing) values will be treated as zero.\n",
    "- If the data are all NA, the result will be 0.\n",
    "- Cumulative methods like `cumsum()` and `cumprod()` ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include NA values, use `skipna=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe98240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94021b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"one\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafbff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.cumsum(skipna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a9e22",
   "metadata": {},
   "source": [
    "### 4. Cleaning / filling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5995e7e9",
   "metadata": {},
   "source": [
    "pandas objects are equipped with various data manipulation methods for dealing with missing data.\n",
    "- `fillna()` can “fill in” NA values with non-NA data in a couple of ways, which we illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5858469",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ad0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"one\"].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f9e19",
   "metadata": {},
   "source": [
    "### 5.Dropping axis labels with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485b312",
   "metadata": {},
   "source": [
    "You may wish to simply exclude labels from a data set which refer to missing data. To do this, use `dropna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c4c99f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df2.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5774a",
   "metadata": {},
   "source": [
    "# Title: pandas.DataFrame.cumsum\n",
    "- Name: Yixuan Feng\n",
    "- Email: fengyx@umich.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68331f5c",
   "metadata": {},
   "source": [
    "## pandas.DataFrame.cumsum\n",
    "- Cumsum is the cumulative function of pandas, used to return the cumulative values of columns or rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70667e72",
   "metadata": {},
   "source": [
    "## Example 1 - Without Setting Parameters\n",
    "- This function will automatically return the cumulative value of all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701804d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_1 = np.random.randint(10, size=10) \n",
    "values_2 = np.random.randint(10, size=10) \n",
    "group = ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A'] \n",
    "df = pd.DataFrame({'group':group, 'value_1':values_1, 'value_2':values_2}) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed5dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8169dd48",
   "metadata": {},
   "source": [
    "## Example 2 - Setting Parameters\n",
    "- By setting the axis to 1, this function will return the cumulative value of all rows.\n",
    "- By combining with groupby() function, other columns (or rows) can be used as references for cumulative addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cumsum_2'] = df[['group', 'value_2']].groupby('group').cumsum() \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c8c7b",
   "metadata": {},
   "source": [
    "[link](https://github.com/fyx1009/Stats507/blob/main/pandas_notes/pd_topic_fengyx.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0d4db",
   "metadata": {},
   "source": [
    "## Stack and Unstack\n",
    "**Heather Johnston**\n",
    "\n",
    "**hajohns@umich.edu**\n",
    "\n",
    "*Stats 507, Pandas Topics, Fall 2021*\n",
    "\n",
    "### About stack and unstack\n",
    "* Stack and Unstack are similar to \"melt\" and \"pivot\" methods for transforming data\n",
    "* R users may be familiar with \"pivot_wider\" and \"pivot_longer\" (formerly \"spread\" and \"gather\")\n",
    "* Stack transforms column names to new index and values to column\n",
    "\n",
    "### Example: Stack\n",
    "* Consider the `example` DataFrame below to be measurements of some value taken on different days at different times.\n",
    "* It would be natural to want these to be \"gathered\" into long format, which we can do using `stack`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.DataFrame({\"day\":[\"Monday\", \"Wednesday\", \"Friday\"],\n",
    "                        \"morning\":[4, 5, 6],\n",
    "                        \"afternoon\":[8, 9, 0]})\n",
    "example.set_index(\"day\", inplace=True)\n",
    "print(example)\n",
    "print(example.stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2dd985",
   "metadata": {},
   "source": [
    "### Example: Unstack\n",
    "* Conversely, for displaying data, it's often handy to have it in a wider format\n",
    "* Unstack is especially convenient after using `groupby` on a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3303bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(100)\n",
    "long_data = pd.DataFrame({\"group\":[\"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\"],\n",
    "                          \"program\":[\"x\", \"y\", \"x\", \"y\", \"x\", \"y\", \"x\", \"y\"],\n",
    "                         \"score\":rng.integers(0, 100, 8),\n",
    "                         \"value\":rng.integers(0, 20, 8)\n",
    "                         })\n",
    "long_data.groupby([\"group\", \"program\"]).mean()\n",
    "long_data.groupby([\"group\", \"program\"]).mean().unstack()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "markdown"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
